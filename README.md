## Nairobi Real Estate Data Pipeline

A real-time data pipeline that scrapes, processes, stores, and visualizes property listings in Nairobi, Kenya â€” starting with platforms like **A-List Real Estate**, **BuyRentKenya**, and **Property24**.

---

##  Project Objective

To build a scalable, modular data pipeline that:
- Collects real estate listings from top Kenyan websites
- Cleans and stores listings in a PostgreSQL database
- Sends scheduled email alerts every 6 hours
- Visualizes trends with Elasticsearch and Kibana

ğŸ“ **Initial focus is on Nairobi County**, with plans to expand to other towns such as Mombasa, Kisumu, and Eldoret.

---
## System Architecture
```

   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ A-List KE  â”‚   â”‚ BuyRentKenya  â”‚   â”‚ Property24 â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
        â–¼                â–¼                   â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Scrapers (Python + Airflow DAGs scheduled via cron)   â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Kafka (raw_listings) â”‚ â—„â”€â”€â”€â”€ JSON scraped data
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ Spark Structured Streaming (Kafka consumer)                  â”‚
     â”‚ - Cleans, deduplicates, transforms                           â”‚
     â”‚ - Automates parsing, type casting, deduplication, formattingâ”‚
     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PostgreSQL: raw_db   â”‚    â”‚ PostgreSQL: clean_db           â”‚
â”‚ - raw.property_listings â”‚    â”‚ - clean.property_listings     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â–¼                          â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Airflow Email Task (every 6h) â”‚   â”‚ Kibana + Elasticsearch (ğŸ“Š UI)â”‚
 â”‚ - Summary of new listings     â”‚   â”‚ - Real-time insights/search  â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```


##  Tech Stack

| Tool        | Role                                      |
|-------------|-------------------------------------------|
| **Python**           | Scrapers, Email Sender, Kafka Producer       |
|  **PostgreSQL**       | Main relational storage                     |
|  **Apache Kafka**     | Real-time ingestion of scraped listings     |
|  **Apache Spark**      | Structured streaming + data transformation |
|  **Apache Airflow**    | DAG scheduling for scraping & notification  |
|  **SMTP / SendGrid**   | Email delivery of new listings summary      |
|  **Elasticsearch + Kibana** ** | Dashboards and map-based search |

Also integrated with:
-  **Docker Compose** for local orchestration
-  **GitHub Actions** for CI/CD automation

---

## Project Structure
```
Nairobi-real-estate-pipeline/
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ dags/
â”‚       â”œâ”€â”€ a_list_scraper_dag.py
â”‚       â”œâ”€â”€ buyrentkenya_scraper_dag.py
â”‚       â”œâ”€â”€ property24_scraper_dag.py
â”‚       â”œâ”€â”€ email_notification_dag.py
â”‚       â””â”€â”€ db_cleanup_or_reprocess_dag.py  # Optional: raw â clean reprocess
â”‚
â”œâ”€â”€ kafka/
â”‚   â””â”€â”€ producer/
â”‚       â””â”€â”€ push_to_kafka.py
â”‚
â”œâ”€â”€ scraper/
â”‚   â”œâ”€â”€ a_list_scraper.py
â”‚   â”œâ”€â”€ buyrentkenya_scraper.py
â”‚   â”œâ”€â”€ property24_scraper.py
â”‚
â”œâ”€â”€ spark/
â”‚   â””â”€â”€ kafka_stream_to_postgres.py
â”‚       # Reads from Kafka â writes to clean_db
â”‚
â”œâ”€â”€ postgres/
â”‚   â”œâ”€â”€ raw_db/
â”‚   â”‚   â”œâ”€â”€ init.sql            # raw.property_listings schema
â”‚   â”‚   â””â”€â”€ docker-entrypoint.sh
â”‚   â”œâ”€â”€ clean_db/
â”‚   â”‚   â”œâ”€â”€ init.sql            # clean.property_listings schema
â”‚   â”‚   â””â”€â”€ docker-entrypoint.sh
â”‚
â”œâ”€â”€ notification/
â”‚   â””â”€â”€ email_notifier.py
â”‚
â”œâ”€â”€ elasticsearch/
â”‚   â”œâ”€â”€ pipeline_to_es.py       # Push transformed data to Elasticsearch
â”‚   â””â”€â”€ dashboards/
â”‚       â””â”€â”€ kibana_saved_objects.ndjson
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

```

## Getting Started
1. Clone the repository
```
git clone https://github.com/samwanyua/Nairobi-real-estate-data-pipeline.git
cd Nairobi-real-estate-data-pipeline
```
2. Configure environment variables
Copy and edit the .env file:

```
cp .env.example .env
```
Example .env:

```
POSTGRES_USER=realestate
POSTGRES_PASSWORD=password
POSTGRES_DB=properties
SMTP_USER=your_email@example.com
SMTP_PASS=your_app_password
EMAIL_RECEIVER=receiver@example.com
```
3. Launch services via Docker
```
docker-compose up -d
```
4. Run a scraper manually (optional)
```
python scraper/a_list_scraper.py
```
5. Start the Spark streaming job
```
spark-submit spark/kafka_stream_to_postgres.py
```
